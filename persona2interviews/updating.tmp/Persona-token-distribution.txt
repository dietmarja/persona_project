Yes, running multiple interviews with a persona and recording the distribution of words or tokens can serve as an approximation of the token distribution for that persona in a system like ChatGPT. However, it won't give you the exact internal token distribution used by the model during inference, but it will provide an empirical proxy for the model's output distribution under specific conditions (i.e., the persona).Steps to Approximate Token Distribution for a Persona:Setup Persona-Specific Prompts:Create or select a persona by providing a specific prompt or instruction to the model (e.g., "Act as a medical expert").Run multiple interview sessions or conversations with the model, ensuring that the persona prompt remains the same to maintain consistency.Collect the Generated Texts:Record the text outputs from each session in full. Itâ€™s essential to have a diverse set of inputs or interview questions to allow the model to exhibit a wide range of its token use.The more samples you collect, the closer your empirical estimate will be to the actual token distribution under that persona.Tokenize the Generated Texts:Use the same tokenizer that the model uses (e.g., Byte Pair Encoding or WordPiece) to break down the collected text into tokens.You can find tokenizers in libraries like transformers by Hugging Face, which allow you to tokenize the generated output using the exact method the model employs.Analyze the Token Frequency:Compute the frequency of each token across all the collected responses.This frequency distribution gives you an approximation of the token distribution for that persona based on the model's generated outputs.Compare with Other Personas:Repeat the process for other personas (or the default behavior of the LLM) to compare token distributions.You can observe how the token usage shifts depending on the persona and compare which words or tokens are more prevalent when different personas are active.Considerations:Sample Size: The accuracy of this approximation depends on the size of your sample. A larger number of generated texts across various prompts will provide a better approximation.Diversity of Input Prompts: Ensure that your input prompts are varied and cover different scenarios within the context of the persona. This will give a more comprehensive view of the token distribution.Sampling Bias: The approximation may reflect only the surface-level patterns of the language model's output and not necessarily the internal token probabilities used by the model during inference.Limitations:Internal vs. External Distribution: The token distribution of generated outputs (what you can observe) may differ from the internal token distribution the model uses to calculate probabilities during generation. The internal distribution is likely more fine-grained and depends on hidden layers that adjust during sampling.Contextual Shifts: The token distribution may vary depending on the specific prompts or questions you ask. If your input prompts are too narrow, you may not capture the full token distribution used by the persona.Empirical Example:If you interview a medical persona multiple times, you would likely see an increased frequency of medical terms (tokens like "diagnosis," "treatment," "symptoms") compared to general-purpose models or other personas. By comparing this frequency distribution with other personas, you can empirically observe the shift in token distributions between them.In conclusion, recording word/token distributions through repeated interviews with a persona can serve as a good approximation of the token distribution, but it will only provide a surface-level view of the model's behavior for that persona. For a more exact understanding, deeper access to the model's internal mechanics would be required, which may not be feasible for closed models like ChatGPT.
