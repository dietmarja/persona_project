# token_analysis/token_distribution_analyzer.pyimport pandas as pdimport mathdef load_token_distribution(file_path):    return pd.read_csv(file_path)def calculate_entropy(token_counts):    total = sum(token_counts.values())    probabilities = [count / total for count in token_counts.values()]    return -sum(p * math.log2(p) for p in probabilities if p > 0)def compare_personas(persona1_tokens, persona2_tokens):    # Simple comparison of common vs unique tokens between two personas    persona1_set = set(persona1_tokens.keys())    persona2_set = set(persona2_tokens.keys())    common_tokens = persona1_set.intersection(persona2_set)    unique_to_p1 = persona1_set - persona2_set    unique_to_p2 = persona2_set - persona1_set    return common_tokens, unique_to_p1, unique_to_p2
