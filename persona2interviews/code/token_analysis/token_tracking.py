# token_analysis/token_tracking.pyfrom collections import Counterclass TokenTracker:    def __init__(self):        self.token_counters = {}    def initialize_token_counters(self, personas):        self.token_counters = {persona['expert']: Counter() for persona in personas}    def track_tokens(self, persona_name, response):        tokens = response.split()  # Simple tokenization for now; could be enhanced        self.token_counters[persona_name].update(tokens)    def get_token_distribution(self):        return self.token_counters    def save_token_distribution(self, file_path):        with open(file_path, 'w') as f:            f.write("Persona,Token,Count")            for persona, counter in self.token_counters.items():                for token, count in counter.items():                    f.write(f"{persona},{token},{count}")